---
title: xpath中小爬爬的运用
date: 2021-02-20 16:05:21
tags: 
	Python
---


# 这儿是爬虫部分xpath的初始学习，在我学习完html和css之后，就采用结点的方式去获取网站了，因此这儿不多赘述，后续有精力再去完善相应的内容。

---



<!--more-->





# 爬虫再一次学习

- 先学了urllib3和requests这两个库，具体的笔记和使用参考<C:\coding\anaconda\爬虫\第一个小爬爬>中的两个例子，这里仅仅是用爬虫抓取了网页，还未进行分析处理。

- HTML页面解析:

  - lxml这个库中提供xpath语句可以灵活提取页面内容

  - 方法:

    - 1. ```from lxml impoet etree```
      2. ```etree.HTML()```(生成etree对象)
      3. `xpath()`

      

- xpth语法:

  - 1. 节点名
  2. / 从根节点选取
    3. //选取所有符合条件的节点
    4. .选取当前节点
    5. ..选取当前父节点
    6.  *这个东西很重要啊，代表一切的可能的标签
    7. @选取属性
  - 路径表达式举例：
    - /bookstore   第一个bookstore
    - /bookstore/book   属于bookstore的子元素的所有book元素
    - //book   选取所有book子元素
    - //bookstore//book   处于bookstore元素后代所有的book元素
    - //@lang   选取名字为lang的所有属性  
    - /bookstore/book/text()   属于bookstore的子元素的所有book元素的文本
  - 谓语：
    - /bookstore/book[1]   xpath中索引都是从0开始，与python中一样的
    - //title[@lang]   选取所有名为lang的属性的title元素
    - //title[@lang='eng']
  - 常见用法（直接复制xpath经常失效哦）:
    - 首先先找到一个大的块儿有id或者是class的那种
    - 然后//div[@class=''\][第几块儿].xpath(接着往下找)

- 数据保存为CSV文件格式：

  - 保存信息: 线路名称，站点信息
  
  - 实现步骤:
    - data_save_util.py模块编写：
    
    ```python
    import csv
    import os
    
    def save_to_csv(line,stations):
        is_exist = False
        if os.path.exists('stations.csv'):
            is_exist = True
        with open('station.csv',mode='a',encoding='utf-8',newline='') as csvfile:
            writer  = csv.writer(csvfile)
            if not is_exist:
                writer.writerow(['公交线路','站点'])
            writer.writerow([line,','.join(stations)])
    ```
    
    
    
    - 添加sava_to_csv(line,stations)方法
    - 将处理好爬取出来的列表作为参数传递给writer.writerow文件，注意传递的东西是String哦，所以才有那个join的处理。
  
- 编写页面爬取控制逻辑：

  - 爬取第一层网页（列表页）：
    - 生成第二层爬取目标页面链接列表
  - 便利爬取第二层网页信息：
    - 爬取第二层网页
    - 提取目标信息
    - 保存目标信息





大的启发：

1. scrapy没有你想的那么好用，这两个库的灵活度更高一点。
2. xpath自己手写非常重要，要去找关键的特征点然后分析页面。